{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a94bbef-d30e-4acb-995e-e834714d2383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "config = {\n",
    "    # Data configuration\n",
    "    \"data\": {\n",
    "        \"train_test_split_ratio\": 0.8,\n",
    "        \"random_state\": 42\n",
    "    },\n",
    "    \n",
    "    # Text preprocessing configuration\n",
    "    \"preprocessing\": {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"lemmatize\": True,\n",
    "        \"min_word_length\": 3\n",
    "    },\n",
    "    \n",
    "    # Word2Vec configuration\n",
    "    \"word2vec\": {\n",
    "        \"vector_size\": 100,\n",
    "        \"window\": 5,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 4\n",
    "    },\n",
    "    \n",
    "    # Model training configuration\n",
    "    \"model\": {\n",
    "        \"classifier\": \"RandomForest\",\n",
    "        \"params\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 10,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6793445-eca0-4fc5-95a8-c965fd31a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data( file_path = '../data/multimodal/raw/COMP5329S1A2Dataset/train.csv') -> pd.DataFrame:\n",
    "    \"\"\"Clean raw CSV data and handle malformed lines\"\"\"\n",
    "    correct_lines = []\n",
    "    problematic_lines = []\n",
    "\n",
    "    # Read and process raw file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file, quotechar='\"', delimiter=',', \n",
    "                          doublequote=True, skipinitialspace=True)\n",
    "        \n",
    "        # Process header\n",
    "        header = next(reader)\n",
    "        correct_lines.append(header)\n",
    "        \n",
    "        # Process rows\n",
    "        for line_number, fields in enumerate(reader, start=2):\n",
    "            if len(fields) == 3:\n",
    "                correct_lines.append(fields)\n",
    "            else:\n",
    "                print(f\"Problematic line {line_number}: {fields}\")\n",
    "                problematic_lines.append(fields)\n",
    "\n",
    "    # Fix problematic lines\n",
    "    for fields in problematic_lines:\n",
    "        if len(fields) > 3:\n",
    "            # Merge extra columns into Caption\n",
    "            fields = [fields[0], fields[1], ','.join(fields[2:])]\n",
    "        elif len(fields) < 3:\n",
    "            # Pad missing columns\n",
    "            fields += [''] * (3 - len(fields))\n",
    "        correct_lines.append(fields)\n",
    "\n",
    "    # Create DataFrame with proper types\n",
    "    df = pd.DataFrame(correct_lines[1:], columns=header)\n",
    "    \n",
    "    return df, df[\"Labels\"].tolist(), df[\"Caption\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ed5f90-655b-491d-b4bf-31f93d6be13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic line 9086: ['9084.jpg', '3 1 11', 'A street sign labeled Seltzer Way', ' with a red fire hydrant in the foreground and a street stretching into the background.']\n",
      "Problematic line 9510: ['9508.jpg', '1', 'A cow in street with writing that reads oh no', 'not beef on the menu again!\"\"']\n",
      "Problematic line 18114: ['18112.jpg', '1', 'A small hand is forming thethumbs up', ' signal.']\n",
      "Problematic line 27169: ['27167.jpg', '10', 'A street sign that says Sex St', ' along with a sign saying there is a $350 penalty is you honk.']\n"
     ]
    }
   ],
   "source": [
    "df, y, X = clean_data( file_path = '../data/multimodal/raw/COMP5329S1A2Dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "856ea409-3125-415c-aa17-333956596b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess_text(\n",
    "    texts: List[str],\n",
    "    config: Dict[str, Any]\n",
    ") -> Tuple[List[List[str]], List[Any]]:\n",
    "    \"\"\"\n",
    "    Preprocess text data by performing tokenization, stopwords removal, and lemmatization.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of raw text strings to be processed\n",
    "        config: Configuration dictionary with preprocessing parameters\n",
    "        \n",
    "    Returns:\n",
    "        processed_texts: List of lists where each inner list contains preprocessed tokens\n",
    "    \"\"\"\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Get configuration parameters\n",
    "    remove_stopwords = config['preprocessing']['remove_stopwords']\n",
    "    lemmatize = config['preprocessing']['lemmatize']\n",
    "    min_word_length = config['preprocessing']['min_word_length']\n",
    "    \n",
    "    processed_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Filter tokens based on configuration\n",
    "        filtered_tokens = []\n",
    "        for token in tokens:\n",
    "            # Skip short words\n",
    "            if len(token) < min_word_length:\n",
    "                continue\n",
    "                \n",
    "            # Skip stopwords if configured\n",
    "            if remove_stopwords and token in stop_words:\n",
    "                continue\n",
    "                \n",
    "            # Apply lemmatization if configured\n",
    "            if lemmatize:\n",
    "                token = lemmatizer.lemmatize(token)\n",
    "                \n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "        processed_texts.append(filtered_tokens)\n",
    "    \n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf771261-da34-4f09-b925-afcec144597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = preprocess_text(X,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c4ecad0-cd10-4007-8e9a-8832c4898336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "def encode_with_word2vec(\n",
    "    processed_texts: List[List[str]],\n",
    "    labels: List[Any],\n",
    "    config: Dict[str, Any]\n",
    ") -> Tuple[np.ndarray, List[Any], Any]:\n",
    "    \"\"\"\n",
    "    Encode preprocessed text tokens using Word2Vec model.\n",
    "    \n",
    "    Args:\n",
    "        processed_texts: List of lists where each inner list contains preprocessed tokens\n",
    "        labels: List of corresponding labels for the texts\n",
    "        config: Configuration dictionary with Word2Vec parameters\n",
    "        \n",
    "    Returns:\n",
    "        text_vectors: NumPy array of text vectors\n",
    "        labels: The original labels passed through\n",
    "        model: Trained Word2Vec model\n",
    "    \"\"\"\n",
    "    # Get Word2Vec configuration parameters\n",
    "    vector_size = config['word2vec']['vector_size']\n",
    "    window = config['word2vec']['window']\n",
    "    min_count = config['word2vec']['min_count']\n",
    "    workers = config['word2vec']['workers']\n",
    "    \n",
    "    # Train Word2Vec model on the preprocessed texts\n",
    "    model = Word2Vec(\n",
    "        sentences=processed_texts,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers\n",
    "    )\n",
    "    \n",
    "    # Create document vectors by averaging word vectors for each document\n",
    "    text_vectors = []\n",
    "    for tokens in processed_texts:\n",
    "        # Filter tokens that are in the model's vocabulary\n",
    "        valid_tokens = [token for token in tokens if token in model.wv]\n",
    "        \n",
    "        if valid_tokens:\n",
    "            # Calculate the average vector for all valid tokens\n",
    "            doc_vector = np.mean([model.wv[token] for token in valid_tokens], axis=0)\n",
    "        else:\n",
    "            # If no valid tokens, use a zero vector\n",
    "            doc_vector = np.zeros(vector_size)\n",
    "            \n",
    "        text_vectors.append(doc_vector)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    text_vectors = np.array(text_vectors)\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    encoded_labels = mlb.fit_transform(labels)\n",
    "    \n",
    "    return text_vectors, encoded_labels, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bb37144-8329-4a70-82eb-8ca38b5faf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mcollecting all words and their counts\u001b[0m\n",
      "\u001b[1;35mPROGRESS: at sentence #0, processed 0 words, keeping 0 word types\u001b[0m\n",
      "\u001b[1;35mPROGRESS: at sentence #10000, processed 58212 words, keeping 3655 word types\u001b[0m\n",
      "\u001b[1;35mPROGRESS: at sentence #20000, processed 116173 words, keeping 4984 word types\u001b[0m\n",
      "\u001b[1;35mcollected 6007 word types from a corpus of 174596 raw words and 30000 sentences\u001b[0m\n",
      "\u001b[1;35mCreating a fresh vocabulary\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'msg': 'effective_min_count=1 retains 6007 unique words (100.00% of original 6007, drops 0)', 'datetime': '2025-04-02T11:39:41.001030', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 174596 word corpus (100.00% of original 174596, drops 0)', 'datetime': '2025-04-02T11:39:41.002030', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\u001b[0m\n",
      "\u001b[1;35mdeleting the raw counts dictionary of 6007 items\u001b[0m\n",
      "\u001b[1;35msample=0.001 downsamples 85 most-common words\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'msg': 'downsampling leaves estimated 135013.73835186547 word corpus (77.3%% of prior 174596)', 'datetime': '2025-04-02T11:39:41.071699', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\u001b[0m\n",
      "\u001b[1;35mestimated required memory for 6007 words and 100 dimensions: 7809100 bytes\u001b[0m\n",
      "\u001b[1;35mresetting layer weights\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-04-02T11:39:41.284941', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'build_vocab'}\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'msg': 'training model with 4 workers on 6007 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-04-02T11:39:41.286849', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\u001b[0m\n",
      "\u001b[1;35mEPOCH 0: training on 174596 raw words (135015 effective words) took 0.2s, 600396 effective words/s\u001b[0m\n",
      "\u001b[1;35mEPOCH 1: training on 174596 raw words (134837 effective words) took 0.2s, 543705 effective words/s\u001b[0m\n",
      "\u001b[1;35mEPOCH 2: training on 174596 raw words (135239 effective words) took 0.2s, 613917 effective words/s\u001b[0m\n",
      "\u001b[1;35mEPOCH 3: training on 174596 raw words (135185 effective words) took 0.2s, 570866 effective words/s\u001b[0m\n",
      "\u001b[1;35mEPOCH 4: training on 174596 raw words (135041 effective words) took 0.2s, 589885 effective words/s\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'msg': 'training on 872980 raw words (675317 effective words) took 1.4s, 497757 effective words/s', 'datetime': '2025-04-02T11:39:42.644500', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\u001b[0m\n",
      "\u001b[1;35mWord2Vec lifecycle event {'params': 'Word2Vec<vocab=6007, vector_size=100, alpha=0.025>', 'datetime': '2025-04-02T11:39:42.645032', 'gensim': '4.3.3', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text_vectors, encoded_labels, model = encode_with_word2vec(\n",
    "    processed_texts,y, config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39d44190-d337-4009-a334-b4200aaba09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 11)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9684187-cb05-4a09-a8a1-b395d73f6b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00739787,  0.22850467, -0.15060474, ..., -0.34482223,\n",
       "         0.1946074 , -0.11335909],\n",
       "       [-0.31826395,  0.13761549,  0.23117161, ..., -0.47330028,\n",
       "         0.07190013, -0.8259868 ],\n",
       "       [-0.05473959,  0.15083836,  0.14118056, ..., -0.26110846,\n",
       "         0.14016667, -0.47439378],\n",
       "       ...,\n",
       "       [-0.05223429,  0.2943649 ,  0.12121125, ..., -0.17811461,\n",
       "         0.1443304 , -0.30702555],\n",
       "       [ 0.00570911,  0.205766  , -0.06081741, ..., -0.2126519 ,\n",
       "         0.1842104 , -0.04316916],\n",
       "       [-0.09749375,  0.3797511 ,  0.10232753, ..., -0.08841111,\n",
       "         0.16017438, -0.42716545]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a487b5d-33d8-43c4-b963-576225272fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def train_model(\n",
    "    text_vectors: np.ndarray,\n",
    "    labels: List[Any],\n",
    "    config: Dict[str, Any]\n",
    ") -> Tuple[Any, float, str]:\n",
    "    \"\"\"\n",
    "    Train a classification model on the Word2Vec encoded text vectors.\n",
    "    \n",
    "    Args:\n",
    "        text_vectors: NumPy array of text vectors\n",
    "        labels: List of corresponding labels for the texts\n",
    "        config: Configuration dictionary with model parameters\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained classification model\n",
    "        accuracy: Model accuracy on test set\n",
    "        report: Classification report as string\n",
    "    \"\"\"\n",
    "    # Get model configuration parameters\n",
    "    classifier_type = config['model']['classifier']\n",
    "    model_params = config['model']['params']\n",
    "    train_test_ratio = config['data']['train_test_split_ratio']\n",
    "    random_state = config['data']['random_state']\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        text_vectors, \n",
    "        labels, \n",
    "        train_size=train_test_ratio,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Initialize the classifier based on configuration\n",
    "    if classifier_type == 'RandomForest':\n",
    "        model = RandomForestClassifier(**model_params)\n",
    "    elif classifier_type == 'LogisticRegression':\n",
    "        model = LogisticRegression(**model_params)\n",
    "    elif classifier_type == 'SVM':\n",
    "        model = SVC(**model_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported classifier type: {classifier_type}\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    return model, accuracy, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f0355dc-43dd-4ccf-9264-8a9c411e2dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alifr\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model, accuracy, report = train_model(\n",
    "    text_vectors,encoded_labels,config\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bf21b56-9aeb-4194-9dba-9a8d6d1162b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.72      0.54      0.62      2114\\n           1       0.76      0.22      0.34       306\\n           2       0.94      0.99      0.96      5491\\n           3       0.00      0.00      0.00       210\\n           4       0.69      0.35      0.46       930\\n           5       0.85      0.30      0.45       277\\n           6       0.99      0.38      0.55       648\\n           7       0.96      0.36      0.52       492\\n           8       0.96      0.70      0.81       517\\n           9       0.93      0.14      0.24       744\\n          10       0.95      0.36      0.53       429\\n\\n   micro avg       0.89      0.66      0.76     12158\\n   macro avg       0.80      0.39      0.50     12158\\nweighted avg       0.86      0.66      0.71     12158\\n samples avg       0.93      0.79      0.81     12158\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36d2202-c28e-4578-abcd-20c4b3923e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
